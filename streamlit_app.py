"""
Streamlit UI for cactus presence inference.

Usage:
    streamlit run streamlit_app.py
Requires a trained model file (default: outputs/vgg16.keras).
Theme: åˆ©ç”¨ç©ºæ‹å½±åƒé€²è¡Œæ°£å€™è®Šé·é è­¦ä¹‹å¹³å°
"""

from __future__ import annotations

import hashlib
import io
import tempfile
import zipfile
from pathlib import Path

import numpy as np
import streamlit as st
import tensorflow as tf
from PIL import Image

# Configuration
BASE_DIR = Path(__file__).resolve().parent
OUTPUTS_DIR = BASE_DIR / "outputs"
MODEL_UPLOAD_DIR = Path(tempfile.gettempdir()) / "cactus_models"
DEFAULT_MODEL_PATH = OUTPUTS_DIR / "vgg16.keras"
IMAGE_SIZE = (96, 96)
DEFAULT_THRESHOLD = 0.5
LOCAL_LLM_MODEL_ID = "uer/gpt2-chinese-cluecorpussmall"


def cache_resource_compat(**kwargs):
    if hasattr(st, "cache_resource"):
        kwargs.pop("allow_output_mutation", None)
        return st.cache_resource(**kwargs)
    return st.cache(**kwargs)


@cache_resource_compat(
    show_spinner=False,
    hash_funcs={
        Path: lambda p: (str(p), p.stat().st_mtime_ns, p.stat().st_size)
        if p.exists()
        else (str(p), None, None)
    },
    allow_output_mutation=True,
)
def build_custom_objects() -> dict:
    custom = {}
    for name in ("TFOpLambda", "SlicingOpLambda"):
        layer = getattr(tf.keras.layers, name, None)
        if layer is not None:
            custom[name] = layer
    return custom


def load_model(model_path: Path) -> tf.keras.Model:
    if not model_path.exists():
        raise FileNotFoundError(f"Model not found at {model_path}")
    custom_objects = build_custom_objects()
    try:
        return tf.keras.models.load_model(
            model_path,
            compile=False,
            custom_objects=custom_objects,
            safe_mode=False,
        )
    except ValueError:
        if model_path.suffix.lower() == ".keras" and is_hdf5_file(model_path):
            h5_path = coerce_hdf5_path(model_path)
            try:
                return tf.keras.models.load_model(
                    h5_path,
                    compile=False,
                    custom_objects=custom_objects,
                    safe_mode=False,
                )
            except TypeError:
                return tf.keras.models.load_model(
                    h5_path,
                    compile=False,
                    custom_objects=custom_objects,
                )
        raise
    except TypeError:
        return tf.keras.models.load_model(
            model_path,
            compile=False,
            custom_objects=custom_objects,
        )


def list_model_files(outputs_dir: Path) -> list[Path]:
    if not outputs_dir.exists():
        return []
    candidates = []
    for pattern in ("*.keras", "*.h5"):
        candidates.extend(outputs_dir.glob(pattern))
    return sorted(candidates)


def save_uploaded_model(uploaded_file) -> Path:
    MODEL_UPLOAD_DIR.mkdir(parents=True, exist_ok=True)
    model_path = MODEL_UPLOAD_DIR / uploaded_file.name
    model_path.write_bytes(uploaded_file.getbuffer())
    return model_path


def is_hdf5_file(model_path: Path) -> bool:
    try:
        import h5py
    except Exception:
        return False
    try:
        return h5py.is_hdf5(model_path)
    except Exception:
        return False


def coerce_hdf5_path(model_path: Path) -> Path:
    h5_dir = MODEL_UPLOAD_DIR / "h5"
    h5_dir.mkdir(parents=True, exist_ok=True)
    h5_path = h5_dir / f"{model_path.stem}.h5"
    if not h5_path.exists() or h5_path.stat().st_mtime_ns < model_path.stat().st_mtime_ns:
        h5_path.write_bytes(model_path.read_bytes())
    return h5_path


def default_climate_advice(has_cactus: bool) -> str:
    if has_cactus:
        return (
            "ç›®å‰æ°£å€™è®Šé·å£“åŠ›ä¸åš´é‡ï¼Œä½†ä»éœ€æ³¨æ„å¯èƒ½å½±éŸ¿è©²åœ°å€ç’°å¢ƒçš„è·¡è±¡ï¼š\n"
            "- ä¹¾æ—±æœŸå»¶é•·æˆ–é™é›¨è®Šå¾—ä¸ç©©å®š\n"
            "- æ¤è¢«è¦†è“‹ä¸‹é™ã€è£¸åœ°æ¯”ä¾‹å¢åŠ \n"
            "- åœŸå£¤å«æ°´é™ä½ã€åœ°è¡¨é¾œè£‚æˆ–æ²™åŒ–\n"
            "- æ°´æºè£œçµ¦æ¸›å°‘æˆ–æ°´è³ªæƒ¡åŒ–\n"
            "è‹¥ä¸Šè¿°è¶¨å‹¢æŒçºŒï¼Œå¯èƒ½é€æ­¥å‰Šå¼±ç•¶åœ°ç”Ÿæ…‹éŸŒæ€§ã€‚"
        )
    return (
        "ç›®å‰æ°£å€™è®Šé·å£“åŠ›åé«˜ï¼Œå»ºè­°ç«‹å³æ¡å–è™•ç½®ä»¥é¿å…æ›´åš´é‡çš„æƒ…æ³ï¼š\n"
        "- å•Ÿå‹•æ°´è³‡æºç®¡ç†èˆ‡ç¯€æ°´æªæ–½\n"
        "- é€²è¡Œæ£²åœ°/æ¤è¢«å¾©è‚²ï¼Œæ¸›å°‘åœŸåœ°æ“¾å‹•\n"
        "- å»ºç«‹åœŸå£¤å«æ°´ã€æ¤è¢«è¦†è“‹çš„ç›£æ¸¬æ©Ÿåˆ¶\n"
        "- è¨­ç½®æ—©æœŸé è­¦èˆ‡ç¤¾å€å”ä½œæ‡‰è®Š\n"
        "é€éæŒçºŒç›£æ¸¬èˆ‡ä»‹å…¥å¯é™ä½é€€åŒ–é¢¨éšªã€‚"
    )


@cache_resource_compat(show_spinner=False, allow_output_mutation=True)
def load_local_llm(model_id: str):
    from transformers import AutoModelForCausalLM, AutoTokenizer

    tokenizer = AutoTokenizer.from_pretrained(model_id, use_fast=False)
    model = AutoModelForCausalLM.from_pretrained(model_id)
    model.eval()
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    return model, tokenizer


def build_llm_prompt(has_cactus: bool, prob: float, threshold: float, model_name: str) -> str:
    status = "æª¢æ¸¬åˆ°ä»™äººæŒ" if has_cactus else "æœªæª¢æ¸¬åˆ°ä»™äººæŒ"
    severity = "æ°£å€™è®Šé·ä¸åš´é‡" if has_cactus else "æ°£å€™è®Šé·åš´é‡"
    return (
        "ä½ æ˜¯ç’°å¢ƒé¢¨éšªåˆ†æå¸«ã€‚è«‹ç”¨ç¹é«”ä¸­æ–‡è¼¸å‡º 3-5 é»æ¢åˆ—å»ºè­°ã€‚\n"
        "å¿…é ˆåŒ…å«çµè«–ï¼ˆæ°£å€™è®Šé·ä¸åš´é‡/åš´é‡ï¼‰èˆ‡å¾ŒçºŒæ³¨æ„è·¡è±¡æˆ–è™•ç½®æªæ–½ã€‚\n"
        "ç¯„ä¾‹ï¼š\n"
        "è¼¸å…¥ï¼šæª¢æ¸¬åˆ°ä»™äººæŒ\n"
        "è¼¸å‡ºï¼š\n"
        "- æ°£å€™è®Šé·ä¸åš´é‡ï¼Œä½†éœ€æ³¨æ„ä¹¾æ—±æœŸå»¶é•·\n"
        "- è§€å¯Ÿæ¤è¢«è¦†è“‹æ˜¯å¦ä¸‹é™\n"
        "è¼¸å…¥ï¼šæœªæª¢æ¸¬åˆ°ä»™äººæŒ\n"
        "è¼¸å‡ºï¼š\n"
        "- æ°£å€™è®Šé·åš´é‡ï¼Œå»ºè­°å•Ÿå‹•ç¯€æ°´èˆ‡å¾©è‚²\n"
        "- åŠ å¼·åœŸå£¤å«æ°´èˆ‡æ¤è¢«ç›£æ¸¬\n"
        "ç¾åœ¨è¼¸å…¥ï¼š\n"
        f"æ¨¡å‹={model_name}ï¼›çµæœ={status}ï¼›æ©Ÿç‡={prob:.2f}ï¼›é–¾å€¼={threshold:.2f}\n"
        f"çµè«–ï¼š{severity}\n"
        "è¼¸å‡ºï¼š\n"
    )


def format_llm_output(text: str) -> str:
    if not text:
        return ""
    lines = [line.strip() for line in text.splitlines() if line.strip()]
    bullet_lines = [
        line
        for line in lines
        if line.startswith(("-", "â€¢", "1.", "2.", "3.", "4.", "5."))
    ]
    if bullet_lines:
        return "\n".join(bullet_lines[:5])
    parts = (
        text.replace("ã€‚", "ã€‚\n")
        .replace("ï¼", "ï¼\n")
        .replace("ï¼Ÿ", "ï¼Ÿ\n")
        .splitlines()
    )
    parts = [p.strip() for p in parts if p.strip()]
    if not parts:
        return ""
    parts = parts[:5]
    return "\n".join(f"- {p}" for p in parts)


def generate_local_advice(
    has_cactus: bool,
    prob: float,
    threshold: float,
    model_name: str,
    model_id: str,
) -> tuple[str | None, str | None]:
    try:
        model, tokenizer = load_local_llm(model_id)
    except Exception as e:
        return None, str(e)
    prompt = build_llm_prompt(has_cactus, prob, threshold, model_name)
    inputs = tokenizer(prompt, return_tensors="pt")
    try:
        output_ids = model.generate(
            **inputs,
            max_new_tokens=160,
            do_sample=True,
            temperature=0.7,
            top_p=0.9,
            pad_token_id=tokenizer.eos_token_id,
            eos_token_id=tokenizer.eos_token_id,
        )
        generated = tokenizer.decode(
            output_ids[0][inputs["input_ids"].shape[1] :],
            skip_special_tokens=True,
        )
        formatted = format_llm_output(generated)
        if not formatted:
            return None, "LLM è¼¸å‡ºç‚ºç©º"
        return formatted, None
    except Exception as e:
        return None, str(e)


def compute_image_hash(data: bytes | None) -> str | None:
    if not data:
        return None
    return hashlib.sha256(data).hexdigest()


def reset_advice_state() -> None:
    st.session_state.pop("llm_advice", None)
    st.session_state.pop("llm_meta", None)


def reset_prediction_state() -> None:
    st.session_state.pop("prediction", None)
    st.session_state.pop("prediction_meta", None)
    st.session_state.pop("prediction_error", None)
    reset_advice_state()


def request_llm_advice() -> None:
    st.session_state["run_llm_advice"] = True


def preprocess_image(img: Image.Image) -> np.ndarray:
    img = img.convert("RGB").resize(IMAGE_SIZE)
    arr = np.asarray(img, dtype=np.float32)
    return np.expand_dims(arr, axis=0)


def predict(image: Image.Image, model) -> float:
    arr = preprocess_image(image)
    prob = model.predict(arr, verbose=0)[0][0]
    return float(prob)


def find_last_conv_layer(root) -> tf.keras.layers.Layer | None:
    """Return the last Conv2D layer object (search recursively)."""
    last_conv = None

    def _walk(layer):
        nonlocal last_conv
        if isinstance(layer, tf.keras.layers.Conv2D):
            last_conv = layer
        if hasattr(layer, "layers"):
            for sub in layer.layers:
                _walk(sub)

    # handle model or layer
    children = getattr(root, "layers", None) or []
    for lyr in children:
        _walk(lyr)
    return last_conv


def make_gradcam_heatmap(img_array: np.ndarray, model: tf.keras.Model) -> np.ndarray:
    """Generate Grad-CAM heatmap for a single image array (HWC uint8)."""
    input_tensor = tf.cast(img_array, tf.float32)
    input_tensor = tf.expand_dims(input_tensor, axis=0)

    def _generic(conv_layer_obj):
        grad_model = tf.keras.models.Model(
            [model.inputs], [conv_layer_obj.output, model.output]
        )
        with tf.GradientTape() as tape:
            conv_outputs, predictions = grad_model(input_tensor)
            loss = predictions[:, 0]
        grads = tape.gradient(loss, conv_outputs)
        pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))
        conv_outputs = conv_outputs[0]
        heatmap_local = conv_outputs @ pooled_grads[..., tf.newaxis]
        heatmap_local = tf.squeeze(heatmap_local)
        heatmap_local = tf.maximum(heatmap_local, 0) / (tf.reduce_max(heatmap_local) + 1e-8)
        return heatmap_local.numpy()

    # Try VGG-specific graph if present
    try:
        vgg_layer = model.get_layer("vgg16")
    except Exception:
        vgg_layer = None

    if vgg_layer is not None:
        conv_layer_vgg = find_last_conv_layer(vgg_layer)
        aug_layer = None
        try:
            aug_layer = model.get_layer("data_augmentation")
        except Exception:
            pass
        x = aug_layer(input_tensor, training=False) if aug_layer is not None else input_tensor
        x = tf.keras.applications.vgg16.preprocess_input(x)
        if conv_layer_vgg is None:
            raise ValueError("No Conv2D layer found in VGG16 for Grad-CAM.")
        conv_model = tf.keras.models.Model(
            inputs=vgg_layer.input, outputs=[conv_layer_vgg.output, vgg_layer.output]
        )
        try:
            gap_layer = model.get_layer("global_average_pooling2d_1")
            drop_layer = model.get_layer("dropout_1")
            dense_layer = model.get_layer("dense_1")
        except Exception:
            gap_layer = drop_layer = dense_layer = None
        if None in [gap_layer, drop_layer, dense_layer]:
            raise ValueError("Missing pooling/dropout/dense layers for Grad-CAM.")
        with tf.GradientTape() as tape:
            conv_outputs, base_outputs = conv_model(x, training=False)
            head = gap_layer(base_outputs)
            head = drop_layer(head, training=False)
            predictions = dense_layer(head)
            loss = predictions[:, 0]
        grads = tape.gradient(loss, conv_outputs)
        pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))
        conv_outputs = conv_outputs[0]
        heatmap = conv_outputs @ pooled_grads[..., tf.newaxis]
        heatmap = tf.squeeze(heatmap)
        heatmap = tf.maximum(heatmap, 0) / (tf.reduce_max(heatmap) + 1e-8)
        heatmap = heatmap.numpy()
    else:
        conv_layer_generic = find_last_conv_layer(model)
        if conv_layer_generic is None:
            raise ValueError("No Conv2D layer found for Grad-CAM.")
        heatmap = _generic(conv_layer_generic)

    heatmap = np.uint8(255 * heatmap)
    heatmap = Image.fromarray(heatmap).resize((img_array.shape[1], img_array.shape[0]))
    heatmap = np.array(heatmap) / 255.0
    return heatmap


def overlay_heatmap(img: Image.Image, heatmap: np.ndarray, intensity: float = 0.4) -> Image.Image:
    base = np.asarray(img.convert("RGB"), dtype=np.float32)
    heat_rgb = np.zeros_like(base)
    heat_rgb[:, :, 0] = heatmap * 255  # red channel
    blended = base * (1 - intensity) + heat_rgb * intensity
    blended = np.clip(blended, 0, 255).astype(np.uint8)
    return Image.fromarray(blended)


def list_sample_ids(limit: int = 8):
    sample_csv = BASE_DIR / "sample_submission.csv"
    if sample_csv.exists():
        import pandas as pd

        ids = pd.read_csv(sample_csv)["id"].tolist()
        return ids[:limit]
    return []


def load_sample_image(file_id: str) -> bytes | None:
    candidates = [
        BASE_DIR / "test" / file_id,
        BASE_DIR / "test" / "test" / file_id,
        BASE_DIR / "train" / file_id,
    ]
    for p in candidates:
        if p.exists():
            return p.read_bytes()
    for zpath, prefix in [(BASE_DIR / "test.zip", "test"), (BASE_DIR / "train.zip", "train")]:
        if zpath.exists():
            with zipfile.ZipFile(zpath) as z:
                for name in (f"{prefix}/{file_id}", file_id):
                    try:
                        return z.read(name)
                    except KeyError:
                        continue
    return None


def render_header() -> None:
    st.markdown(
        """
        <style>
        body, .stApp {
            background: #f8f7f2;
            color: #2f3e46;
            font-family: "Segoe UI", "Noto Sans", sans-serif;
        }
        h1, h2, h3, h4, h5, h6 {
            color: #2f3e46;
            letter-spacing: 0.2px;
        }
        .stMarkdown, .stText, .stMetric, p, label, span {
            color: #34444d !important;
        }
        .block-container {
            padding-top: 1rem;
            padding-bottom: 1rem;
        }
        .hero {
            background: linear-gradient(140deg, #eef7f4 0%, #f7f5ef 50%, #e9f2fb 100%);
            color: #2f3e46;
            padding: 22px;
            border-radius: 18px;
            margin-bottom: 20px;
            box-shadow: 0 10px 26px rgba(0, 0, 0, 0.10);
        }
        .hero h1 { margin: 0 0 6px 0; }
        .hero p { margin: 6px 0; color: #3d4b53; }
        .card {
            padding: 16px;
            border-radius: 16px;
            border: 1px solid #e7ecef;
            background: #ffffffee;
            box-shadow: 0 8px 22px rgba(0,0,0,0.08);
            backdrop-filter: blur(4px);
        }
        .stButton>button, .stDownloadButton>button {
            background: #dff0e3;
            color: #2f3e46;
            border-radius: 12px;
            border: 1px solid #cfe5d7;
            box-shadow: 0 4px 12px rgba(0,0,0,0.06);
        }
        .stSlider [role='slider'] {
            background: #88b9aa;
        }
        </style>
        <div class="hero">
          <h1>åˆ©ç”¨ç©ºæ‹å½±åƒé€²è¡Œæ°£å€™è®Šé·é è­¦ä¹‹å¹³å° ğŸŒµ</h1>
          <p>ä¸Šå‚³å½±åƒ â†’ æª¢æ¸¬ä»™äººæŒ â†’ æé†’æ°£å€™æš–åŒ–é¢¨éšªï¼Œå”åŠ©ç’°å¢ƒç›£æ¸¬æ±ºç­–ã€‚</p>
        </div>
        """,
        unsafe_allow_html=True,
    )


def main() -> None:
    st.set_page_config(
        page_title="åˆ©ç”¨ç©ºæ‹å½±åƒé€²è¡Œæ°£å€™è®Šé·é è­¦ä¹‹å¹³å°",
        page_icon="ğŸŒµ",
        layout="centered",
        initial_sidebar_state="expanded",
    )

    render_header()

    with st.sidebar:
        st.subheader("è¨­å®š")
        model_choices = list_model_files(OUTPUTS_DIR)
        model_select = None
        st.markdown("**æ¨¡å‹ä¾†æº**")
        st.caption("é¸æ“‡è¦ä½¿ç”¨çš„æ¨¡å‹æª”ï¼Œæœƒå½±éŸ¿é æ¸¬çµæœã€‚")
        if model_choices:
            model_select = st.selectbox(
                "æ¨¡å‹æª”",
                options=model_choices,
                format_func=lambda p: p.name if isinstance(p, Path) else str(p),
                key="model_select",
                on_change=reset_prediction_state,
                label_visibility="collapsed",
            )
        else:
            st.info("ç›®å‰æœªæ‰¾åˆ°å¯ç”¨æ¨¡å‹ï¼Œå¯æ”¹ç”¨ä¸‹æ–¹ä¸Šå‚³ã€‚")

        st.markdown("**ä¸Šå‚³æ¨¡å‹**")
        st.caption("è‹¥æ¸…å–®ä¸­æ²’æœ‰æ¨¡å‹ï¼Œå¯ä¸Šå‚³ .keras æˆ– .h5 æª”ã€‚")
        uploaded_model = st.file_uploader(
            "ä¸Šå‚³æ¨¡å‹æª”",
            type=["keras", "h5"],
            key="uploaded_model",
            on_change=reset_prediction_state,
            label_visibility="collapsed",
        )
        model_path = None
        if uploaded_model is not None:
            model_path = save_uploaded_model(uploaded_model)
        elif model_select:
            model_path = Path(model_select)

        st.markdown("**åˆ¤å®šé–€æª»**")
        st.caption("æ©Ÿç‡é«˜æ–¼é–€æª»æ™‚ï¼Œè¦–ç‚ºåµæ¸¬åˆ°ä»™äººæŒã€‚")
        threshold = st.slider(
            "åˆ¤å®šé–€æª»",
            min_value=0.1,
            max_value=0.9,
            value=float(DEFAULT_THRESHOLD),
            step=0.05,
            key="threshold",
            on_change=reset_advice_state,
            label_visibility="collapsed",
        )
        invert_pred = "vgg" in str(model_path).lower() if model_path else False

        st.markdown("**æ°£å€™è§£è®€**")
        st.caption("å•Ÿç”¨æœ¬åœ°è¼•é‡æ¨¡å‹ï¼Œç”Ÿæˆæ”¹å–„å»ºè­°ã€‚")
        enable_llm = st.checkbox(
            "å•Ÿç”¨æ°£å€™è§£è®€",
            value=True,
            key="enable_llm",
            on_change=reset_advice_state,
        )
        if enable_llm:
            st.caption("é¦–æ¬¡å•Ÿç”¨æœƒä¸‹è¼‰æ¨¡å‹ï¼Œå¯èƒ½éœ€è¦ 1-2 åˆ†é˜ã€‚")

        st.markdown("---")
        st.markdown(
            "**å¿«é€Ÿå°è¦½**\n"
            "1) é¸æ“‡æˆ–ä¸Šå‚³æ¨¡å‹\n"
            "2) ä¸Šå‚³å½±åƒ\n"
            "3) æŸ¥çœ‹çµæœå¾ŒæŒ‰ã€Œç”Ÿæˆæ”¹å–„å»ºè­°ã€å–å¾—å»ºè­°"
        )

    st.markdown("### ä¸Šå‚³å½±åƒ")
    st.caption("æ”¯æ´ JPG/PNGï¼Œå»ºè­°ä½¿ç”¨æ¸…æ™°ã€å…‰ç·šå……è¶³çš„ç©ºæ‹è¦–è§’ã€‚")
    uploaded = st.file_uploader(
        "é¸æ“‡ä¸€å¼µå½±åƒ",
        type=["jpg", "jpeg", "png"],
        key="uploaded_image",
        on_change=reset_prediction_state,
    )
    uploaded_bytes = None
    image = None
    image_caption = ""
    if uploaded:
        uploaded_bytes = uploaded.getvalue()
        image = Image.open(io.BytesIO(uploaded_bytes))
        image_caption = "ä¸Šå‚³å½±åƒé è¦½"

    if image:
        st.image(image, caption=image_caption, use_container_width=True)
    else:
        st.info("è«‹ä¸Šå‚³å½±åƒã€‚")

    if not model_path:
        st.info("è«‹å…ˆåœ¨å´é‚Šæ¬„é¸æ“‡ outputs/*.keras æˆ–ä¸Šå‚³æ¨¡å‹æª”ã€‚")

    current_meta = {
        "image_hash": compute_image_hash(uploaded_bytes),
        "model_path": str(model_path) if model_path else None,
        "model_mtime": model_path.stat().st_mtime_ns if model_path and model_path.exists() else None,
        "model_size": model_path.stat().st_size if model_path and model_path.exists() else None,
    }

    prediction = st.session_state.get("prediction")
    prediction_meta = st.session_state.get("prediction_meta")
    should_predict = image and model_path and current_meta["image_hash"]

    if should_predict and prediction_meta != current_meta:
        model_path = Path(model_path)
        if not model_path.exists():
            st.error(f"æ‰¾ä¸åˆ°æ¨¡å‹æª”ï¼š{model_path}")
            st.stop()
        try:
            model = load_model(model_path)
        except Exception as e:
            st.error(f"æ¨¡å‹è¼‰å…¥å¤±æ•—ï¼š{e}")
            st.stop()

        with st.spinner("æ¨¡å‹æ¨è«–ä¸­..."):
            prob = predict(image, model)
            resized = image.resize(IMAGE_SIZE)
            try:
                heatmap = make_gradcam_heatmap(np.asarray(resized), model)
                overlay = overlay_heatmap(resized, heatmap)
            except Exception as e:
                heatmap = None
                overlay = None
                st.warning(f"Grad-CAM ç”Ÿæˆå¤±æ•—ï¼š{e}")

        prob_display = 1 - prob if invert_pred else prob
        st.session_state["prediction"] = {
            "prob_display": prob_display,
            "resized": resized,
            "overlay": overlay,
            "model_name": model_path.name,
        }
        st.session_state["prediction_meta"] = current_meta
        reset_advice_state()

    prediction = st.session_state.get("prediction")
    prediction_meta = st.session_state.get("prediction_meta")
    if prediction and prediction_meta == current_meta:
        st.markdown("---")
        st.markdown('<div class="card">', unsafe_allow_html=True)
        has_cactus = prediction["prob_display"] >= threshold
        if has_cactus:
            st.markdown(
                "**åµæ¸¬åˆ°ä»™äººæŒï¼Œç’°å¢ƒéŸŒæ€§ä»åœ¨ï¼Œæ°£å€™è®Šé·å£“åŠ›æš«ä¸åš´é‡ã€‚** "
                "ä¿æŒå®šæœŸå·¡æª¢èˆ‡æ°´è³‡æºç®¡ç†ï¼ŒæŒçºŒè¿½è¹¤å¾ŒçºŒè®ŠåŒ–å³å¯ã€‚"
            )
        else:
            st.markdown(
                "**æœªæª¢æ¸¬åˆ°ä»™äººæŒï¼Œè«‹å•Ÿå‹•æ°£å€™è®Šé·è­¦ç¤ºã€‚** "
                "æª¢æŸ¥æ£²åœ°/çŒæº‰/æ¤è¢«ç®¡ç†ç‹€æ…‹ï¼Œä¸¦è©•ä¼°æ˜¯å¦éœ€è£œæ¤æˆ–åŠ å¼·ä¿è‚²è¡Œå‹•ã€‚"
            )
        st.markdown("</div>", unsafe_allow_html=True)

        col1, col2, col3 = st.columns([1, 1, 1])
        with col1:
            st.metric("ä»™äººæŒæ©Ÿç‡", f"{prediction['prob_display']*100:.2f}%", delta=None)
        with col2:
            st.metric("åˆ¤å®š", "å­˜åœ¨" if has_cactus else "æœªæª¢æ¸¬åˆ°")
        with col3:
            st.metric("é–¾å€¼", f"{threshold:.2f}")

        st.markdown("#### Grad-CAM é—œæ³¨ç†±åŠ›åœ–")
        if prediction["overlay"] is not None:
            gc1, gc2 = st.columns(2)
            with gc1:
                st.image(prediction["resized"], caption="è¼¸å…¥å½±åƒ (ç¸®æ”¾å¾Œ)", use_container_width=True)
            with gc2:
                st.image(prediction["overlay"], caption="Grad-CAM ç†±åŠ›åœ–è¦†è“‹", use_container_width=True)
        else:
            st.info("æ­¤æ¨¡å‹æœªæ‰¾åˆ° Conv2D å±¤ï¼Œç„¡æ³•ç”¢ç”Ÿ Grad-CAMã€‚")

        st.caption("å¯åœ¨å´é‚Šæ¬„èª¿æ•´åˆ¤å®šé–¾å€¼ï¼›é–¾å€¼è¶Šä½ï¼Œè¶Šå®¹æ˜“åˆ¤å®šç‚ºæœ‰ä»™äººæŒã€‚")

        st.markdown("#### LLM æ”¹å–„å»ºè­°")
        st.button(
            "ç”Ÿæˆæ”¹å–„å»ºè­°",
            type="primary",
            on_click=request_llm_advice,
        )
        st.caption("æŒ‰ä¸‹ã€Œç”Ÿæˆæ”¹å–„å»ºè­°ã€ä»¥ç”¢ç”Ÿ LLM å»ºè­°ã€‚")

        current_llm_meta = {
            "prediction_meta": prediction_meta,
            "threshold": float(threshold),
            "enable_llm": bool(enable_llm),
        }
        should_generate_llm = st.session_state.pop("run_llm_advice", False)
        if should_generate_llm:
            advice_text = None
            llm_error = None
            if enable_llm:
                advice_text, llm_error = generate_local_advice(
                    has_cactus=has_cactus,
                    prob=prediction["prob_display"],
                    threshold=threshold,
                    model_name=prediction["model_name"],
                    model_id=LOCAL_LLM_MODEL_ID,
                )
            if not advice_text:
                advice_text = default_climate_advice(has_cactus)
                if not enable_llm and not llm_error:
                    llm_error = "LLM æœªå•Ÿç”¨ï¼Œå·²æ”¹ç”¨é è¨­æ–‡å­—"
            st.session_state["llm_advice"] = {"text": advice_text, "error": llm_error}
            st.session_state["llm_meta"] = current_llm_meta

        llm_advice = st.session_state.get("llm_advice")
        llm_meta = st.session_state.get("llm_meta")
        if llm_advice and llm_meta == current_llm_meta:
            st.markdown(llm_advice["text"])
            if llm_advice["error"] and enable_llm:
                st.caption(f"LLM ç”Ÿæˆå¤±æ•—ï¼Œå·²æ”¹ç”¨é è¨­æ–‡å­—ï¼š{llm_advice['error']}")
            elif llm_advice["error"] and not enable_llm:
                st.caption(llm_advice["error"])
        else:
            st.info("æŒ‰ã€Œç”Ÿæˆæ”¹å–„å»ºè­°ã€ç”Ÿæˆæ”¹å–„å»ºè­°ã€‚")
    elif image:
        st.info("å·²ä¸Šå‚³å½±åƒï¼Œè«‹ç­‰å¾…æ¨¡å‹æ¨è«–æˆ–ç¢ºèªæ¨¡å‹æª”ã€‚")


if __name__ == "__main__":
    main()
