"""
Streamlit UI for cactus presence inference.

Usage:
    streamlit run streamlit_app.py
Requires a trained model file (default: outputs/vgg16.keras).
Theme: åˆ©ç”¨ç©ºæ‹å½±åƒé€²è¡Œæ°£å€™è®Šé·é è­¦ä¹‹å¹³å°
"""

from __future__ import annotations

import hashlib
import io
import time
import tempfile
import zipfile
from pathlib import Path

import numpy as np
import streamlit as st
import tensorflow as tf
from PIL import Image

# Configuration
BASE_DIR = Path(__file__).resolve().parent
OUTPUTS_DIR = BASE_DIR / "outputs"
MODEL_UPLOAD_DIR = Path(tempfile.gettempdir()) / "cactus_models"
DEFAULT_MODEL_PATH = OUTPUTS_DIR / "vgg16.keras"
IMAGE_SIZE = (96, 96)
DEFAULT_THRESHOLD = 0.5
LOCAL_LLM_MODEL_ID = "uer/gpt2-chinese-cluecorpussmall"
HDF5_SIGNATURE = b"\x89HDF\r\n\x1a\n"
ZIP_SIGNATURES = (b"PK\x03\x04", b"PK\x05\x06", b"PK\x07\x08")
LFS_SIGNATURE = b"version https://git-lfs.github.com/spec/v1"


def cache_resource_compat(**kwargs):
    if hasattr(st, "cache_resource"):
        kwargs.pop("allow_output_mutation", None)
        return st.cache_resource(**kwargs)
    def decorator(func):
        return func
    return decorator


@cache_resource_compat(
    show_spinner=False,
    hash_funcs={
        Path: lambda p: (str(p), p.stat().st_mtime_ns, p.stat().st_size)
        if p.exists()
        else (str(p), None, None)
    },
    allow_output_mutation=True,
)
def build_custom_objects() -> dict:
    custom = {}
    for name in ("TFOpLambda", "SlicingOpLambda"):
        layer = getattr(tf.keras.layers, name, None)
        if layer is not None:
            custom[name] = layer
    return custom


def build_augmentation_layer() -> tf.keras.Sequential:
    return tf.keras.Sequential(
        [
            tf.keras.layers.RandomFlip("horizontal"),
            tf.keras.layers.RandomRotation(0.1),
            tf.keras.layers.RandomZoom(0.1),
        ],
        name="data_augmentation",
    )


def build_cnn_infer(input_shape: tuple[int, int, int]) -> tf.keras.Model:
    aug = build_augmentation_layer()
    inputs = tf.keras.layers.Input(shape=input_shape)
    x = aug(inputs)
    x = tf.keras.layers.Rescaling(1.0 / 255.0)(x)
    x = tf.keras.layers.Conv2D(32, 3, padding="same", activation="relu")(x)
    x = tf.keras.layers.MaxPooling2D()(x)
    x = tf.keras.layers.Conv2D(64, 3, padding="same", activation="relu")(x)
    x = tf.keras.layers.MaxPooling2D()(x)
    x = tf.keras.layers.Conv2D(128, 3, padding="same", activation="relu")(x)
    x = tf.keras.layers.GlobalAveragePooling2D()(x)
    x = tf.keras.layers.Dropout(0.3)(x)
    outputs = tf.keras.layers.Dense(1, activation="sigmoid")(x)
    return tf.keras.models.Model(inputs, outputs, name="simple_cnn")


def build_vgg16_infer(input_shape: tuple[int, int, int]) -> tf.keras.Model:
    aug = build_augmentation_layer()
    base = tf.keras.applications.VGG16(
        include_top=False, weights=None, input_shape=input_shape
    )
    base.trainable = False
    inputs = tf.keras.layers.Input(shape=input_shape)
    x = aug(inputs)
    x = tf.keras.applications.vgg16.preprocess_input(x)
    x = base(x, training=False)
    x = tf.keras.layers.GlobalAveragePooling2D()(x)
    x = tf.keras.layers.Dropout(0.4)(x)
    outputs = tf.keras.layers.Dense(1, activation="sigmoid")(x)
    return tf.keras.models.Model(inputs, outputs, name="vgg16_transfer")


@cache_resource_compat(
    show_spinner=False,
    hash_funcs={
        Path: lambda p: (str(p), p.stat().st_mtime_ns, p.stat().st_size)
        if p.exists()
        else (str(p), None, None)
    },
    allow_output_mutation=True,
)
def load_model(model_path: Path) -> tf.keras.Model:
    if not model_path.exists():
        raise FileNotFoundError(f"Model not found at {model_path}")
    custom_objects = build_custom_objects()
    file_type = detect_model_file_type(model_path)
    if file_type == "lfs":
        raise ValueError("æ¨¡å‹æª”çœ‹èµ·ä¾†æ˜¯ Git LFS æŒ‡æ¨™ï¼Œè«‹ç”¨ git lfs pull æˆ–é‡æ–°ä¸Šå‚³å¯¦é«”æ¨¡å‹æª”ã€‚")
    if file_type == "unknown":
        raise ValueError("æ¨¡å‹æª”æ ¼å¼ç„¡æ³•è¾¨è­˜ï¼Œè«‹ç¢ºèªæª”æ¡ˆæœªææ¯€ä¸¦é‡æ–°è¼¸å‡ºç‚º .h5/.kerasã€‚")
    if file_type == "hdf5":
        resolved_path = resolve_hdf5_path(model_path)
    else:
        resolved_path = model_path

    try:
        return tf.keras.models.load_model(
            resolved_path,
            compile=False,
            custom_objects=custom_objects,
            safe_mode=False,
        )
    except TypeError:
        return tf.keras.models.load_model(
            resolved_path,
            compile=False,
            custom_objects=custom_objects,
        )
    except Exception as e:
        if file_type == "zip":
            raise ValueError(f"ç„¡æ³•è¼‰å…¥ .keras (zip) æ¨¡å‹ï¼Œè«‹ç¢ºèª TensorFlow ç‰ˆæœ¬æˆ–æ”¹å­˜ .h5ï¼š{e}") from e
        raise


def load_model_by_weights(model_path: Path) -> tf.keras.Model:
    name = model_path.name.lower()
    input_shape = (IMAGE_SIZE[0], IMAGE_SIZE[1], 3)
    if "vgg" in name:
        model = build_vgg16_infer(input_shape)
    elif "cnn" in name:
        model = build_cnn_infer(input_shape)
    else:
        raise ValueError("Unsupported model name for weight loading.")
    file_type = detect_model_file_type(model_path)
    if file_type != "hdf5":
        raise ValueError(f"weights-only load requires HDF5, got {file_type}")
    resolved_path = resolve_hdf5_path(model_path)
    try:
        model.load_weights(resolved_path)
    except Exception:
        model.load_weights(resolved_path, by_name=True, skip_mismatch=True)
    return model


def get_cached_model(model_path: Path) -> tf.keras.Model:
    cache = st.session_state.get("model_cache", {})
    key = str(model_path)
    meta = (
        model_path.stat().st_mtime_ns,
        model_path.stat().st_size,
    )
    entry = cache.get(key)
    if entry and entry.get("meta") == meta:
        return entry["model"]
    file_type = detect_model_file_type(model_path)
    start = time.perf_counter()
    try:
        print(f"[model] loading by weights ({file_type}): {model_path}")
        model = load_model_by_weights(model_path)
        method = "weights"
    except Exception as e:
        print(f"[model] weights load failed: {e}; fallback to full load")
        model = load_model(model_path)
        method = "full"
    elapsed = time.perf_counter() - start
    st.session_state["model_load_info"] = f"{method} load {elapsed:.2f}s / {file_type}"
    cache[key] = {"meta": meta, "model": model}
    st.session_state["model_cache"] = cache
    return model


def list_model_files(outputs_dir: Path) -> list[Path]:
    if not outputs_dir.exists():
        return []
    candidates = []
    for pattern in ("*.keras", "*.h5"):
        candidates.extend(outputs_dir.glob(pattern))
    return sorted(candidates)


def save_uploaded_model(uploaded_file) -> Path:
    MODEL_UPLOAD_DIR.mkdir(parents=True, exist_ok=True)
    model_path = MODEL_UPLOAD_DIR / uploaded_file.name
    model_path.write_bytes(uploaded_file.getbuffer())
    return model_path


def looks_like_hdf5(model_path: Path) -> bool:
    try:
        with model_path.open("rb") as handle:
            return handle.read(len(HDF5_SIGNATURE)) == HDF5_SIGNATURE
    except OSError:
        return False


def validate_hdf5_file(model_path: Path) -> bool:
    try:
        import h5py
    except Exception:
        return looks_like_hdf5(model_path)
    try:
        with h5py.File(model_path, "r"):
            return True
    except Exception:
        return False


def detect_model_file_type(model_path: Path) -> str:
    if model_path.is_dir():
        return "saved_model"
    if not model_path.exists():
        return "missing"
    try:
        with model_path.open("rb") as handle:
            head = handle.read(256)
    except OSError:
        return "missing"
    if head.startswith(LFS_SIGNATURE):
        return "lfs"
    if head.startswith(HDF5_SIGNATURE):
        return "hdf5" if validate_hdf5_file(model_path) else "unknown"
    if any(head.startswith(sig) for sig in ZIP_SIGNATURES):
        return "zip"
    try:
        if zipfile.is_zipfile(model_path):
            return "zip"
    except OSError:
        pass
    if is_hdf5_file(model_path) and validate_hdf5_file(model_path):
        return "hdf5"
    return "unknown"


def is_hdf5_file(model_path: Path) -> bool:
    try:
        import h5py
    except Exception:
        return looks_like_hdf5(model_path)
    try:
        return h5py.is_hdf5(model_path)
    except Exception:
        return looks_like_hdf5(model_path)


def coerce_hdf5_path(model_path: Path) -> Path:
    h5_dir = MODEL_UPLOAD_DIR / "h5"
    h5_dir.mkdir(parents=True, exist_ok=True)
    stat = model_path.stat()
    cache_key = f"{model_path.stem}-{stat.st_mtime_ns}-{stat.st_size}"
    h5_path = h5_dir / f"{cache_key}.h5"
    if not h5_path.exists():
        h5_path.write_bytes(model_path.read_bytes())
    return h5_path


def resolve_hdf5_path(model_path: Path) -> Path:
    if model_path.suffix.lower() == ".keras" and is_hdf5_file(model_path):
        return coerce_hdf5_path(model_path)
    return model_path


def default_climate_advice(has_cactus: bool) -> str:
    if has_cactus:
        return (
            "ç›®å‰æ°£å€™è®Šé·å£“åŠ›ä¸åš´é‡ï¼Œä½†ä»éœ€æ³¨æ„å¯èƒ½å½±éŸ¿è©²åœ°å€ç’°å¢ƒçš„è·¡è±¡ï¼š\n"
            "- ä¹¾æ—±æœŸå»¶é•·æˆ–é™é›¨è®Šå¾—ä¸ç©©å®š\n"
            "- æ¤è¢«è¦†è“‹ä¸‹é™ã€è£¸åœ°æ¯”ä¾‹å¢åŠ \n"
            "- åœŸå£¤å«æ°´é™ä½ã€åœ°è¡¨é¾œè£‚æˆ–æ²™åŒ–\n"
            "- æ°´æºè£œçµ¦æ¸›å°‘æˆ–æ°´è³ªæƒ¡åŒ–\n"
            "è‹¥ä¸Šè¿°è¶¨å‹¢æŒçºŒï¼Œå¯èƒ½é€æ­¥å‰Šå¼±ç•¶åœ°ç”Ÿæ…‹éŸŒæ€§ã€‚"
        )
    return (
        "ç›®å‰æ°£å€™è®Šé·å£“åŠ›åé«˜ï¼Œå»ºè­°ç«‹å³æ¡å–è™•ç½®ä»¥é¿å…æ›´åš´é‡çš„æƒ…æ³ï¼š\n"
        "- å•Ÿå‹•æ°´è³‡æºç®¡ç†èˆ‡ç¯€æ°´æªæ–½\n"
        "- é€²è¡Œæ£²åœ°/æ¤è¢«å¾©è‚²ï¼Œæ¸›å°‘åœŸåœ°æ“¾å‹•\n"
        "- å»ºç«‹åœŸå£¤å«æ°´ã€æ¤è¢«è¦†è“‹çš„ç›£æ¸¬æ©Ÿåˆ¶\n"
        "- è¨­ç½®æ—©æœŸé è­¦èˆ‡ç¤¾å€å”ä½œæ‡‰è®Š\n"
        "é€éæŒçºŒç›£æ¸¬èˆ‡ä»‹å…¥å¯é™ä½é€€åŒ–é¢¨éšªã€‚"
    )


@cache_resource_compat(show_spinner=False, allow_output_mutation=True)
def load_local_llm(model_id: str):
    from transformers import AutoModelForCausalLM, AutoTokenizer

    tokenizer = AutoTokenizer.from_pretrained(model_id, use_fast=False)
    model = AutoModelForCausalLM.from_pretrained(model_id)
    model.eval()
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    return model, tokenizer


def get_cached_llm(model_id: str):
    cache = st.session_state.get("llm_cache", {})
    entry = cache.get(model_id)
    if entry:
        return entry
    model, tokenizer = load_local_llm(model_id)
    cache[model_id] = (model, tokenizer)
    st.session_state["llm_cache"] = cache
    return model, tokenizer


def build_llm_prompt(has_cactus: bool, prob: float, threshold: float, model_name: str) -> str:
    status = "æª¢æ¸¬åˆ°ä»™äººæŒ" if has_cactus else "æœªæª¢æ¸¬åˆ°ä»™äººæŒ"
    severity = "æ°£å€™è®Šé·ä¸åš´é‡" if has_cactus else "æ°£å€™è®Šé·åš´é‡"
    return (
        "ä½ æ˜¯ç’°å¢ƒé¢¨éšªåˆ†æå¸«ã€‚è«‹ç”¨ç¹é«”ä¸­æ–‡è¼¸å‡º 3-5 é»æ¢åˆ—å»ºè­°ã€‚\n"
        "å¿…é ˆåŒ…å«çµè«–ï¼ˆæ°£å€™è®Šé·ä¸åš´é‡/åš´é‡ï¼‰èˆ‡å¾ŒçºŒæ³¨æ„è·¡è±¡æˆ–è™•ç½®æªæ–½ã€‚\n"
        "ç¯„ä¾‹ï¼š\n"
        "è¼¸å…¥ï¼šæª¢æ¸¬åˆ°ä»™äººæŒ\n"
        "è¼¸å‡ºï¼š\n"
        "- æ°£å€™è®Šé·ä¸åš´é‡ï¼Œä½†éœ€æ³¨æ„ä¹¾æ—±æœŸå»¶é•·\n"
        "- è§€å¯Ÿæ¤è¢«è¦†è“‹æ˜¯å¦ä¸‹é™\n"
        "è¼¸å…¥ï¼šæœªæª¢æ¸¬åˆ°ä»™äººæŒ\n"
        "è¼¸å‡ºï¼š\n"
        "- æ°£å€™è®Šé·åš´é‡ï¼Œå»ºè­°å•Ÿå‹•ç¯€æ°´èˆ‡å¾©è‚²\n"
        "- åŠ å¼·åœŸå£¤å«æ°´èˆ‡æ¤è¢«ç›£æ¸¬\n"
        "ç¾åœ¨è¼¸å…¥ï¼š\n"
        f"æ¨¡å‹={model_name}ï¼›çµæœ={status}ï¼›æ©Ÿç‡={prob:.2f}ï¼›é–¾å€¼={threshold:.2f}\n"
        f"çµè«–ï¼š{severity}\n"
        "è¼¸å‡ºï¼š\n"
    )


def format_llm_output(text: str) -> str:
    if not text:
        return ""
    lines = [line.strip() for line in text.splitlines() if line.strip()]
    bullet_lines = [
        line
        for line in lines
        if line.startswith(("-", "â€¢", "1.", "2.", "3.", "4.", "5."))
    ]
    if bullet_lines:
        return "\n".join(bullet_lines[:5])
    parts = (
        text.replace("ã€‚", "ã€‚\n")
        .replace("ï¼", "ï¼\n")
        .replace("ï¼Ÿ", "ï¼Ÿ\n")
        .splitlines()
    )
    parts = [p.strip() for p in parts if p.strip()]
    if not parts:
        return ""
    parts = parts[:5]
    return "\n".join(f"- {p}" for p in parts)


def generate_local_advice(
    has_cactus: bool,
    prob: float,
    threshold: float,
    model_name: str,
    model_id: str,
) -> tuple[str | None, str | None]:
    try:
        model, tokenizer = get_cached_llm(model_id)
    except Exception as e:
        return None, str(e)
    prompt = build_llm_prompt(has_cactus, prob, threshold, model_name)
    inputs = tokenizer(prompt, return_tensors="pt")
    try:
        output_ids = model.generate(
            **inputs,
            max_new_tokens=160,
            do_sample=True,
            temperature=0.7,
            top_p=0.9,
            pad_token_id=tokenizer.eos_token_id,
            eos_token_id=tokenizer.eos_token_id,
        )
        generated = tokenizer.decode(
            output_ids[0][inputs["input_ids"].shape[1] :],
            skip_special_tokens=True,
        )
        formatted = format_llm_output(generated)
        if not formatted:
            return None, "LLM è¼¸å‡ºç‚ºç©º"
        return formatted, None
    except Exception as e:
        return None, str(e)


def compute_image_hash(data: bytes | None) -> str | None:
    if not data:
        return None
    return hashlib.sha256(data).hexdigest()


def reset_advice_state() -> None:
    st.session_state.pop("llm_advice", None)
    st.session_state.pop("llm_meta", None)


def reset_gradcam_state() -> None:
    st.session_state.pop("gradcam", None)
    st.session_state.pop("gradcam_meta", None)


def reset_prediction_state() -> None:
    st.session_state.pop("prediction", None)
    st.session_state.pop("prediction_meta", None)
    st.session_state.pop("prediction_error", None)
    reset_advice_state()
    reset_gradcam_state()
    st.session_state.pop("model_cache", None)


def request_llm_advice() -> None:
    st.session_state["run_llm_advice"] = True


def preprocess_image(img: Image.Image) -> np.ndarray:
    img = img.convert("RGB").resize(IMAGE_SIZE)
    arr = np.asarray(img, dtype=np.float32)
    return np.expand_dims(arr, axis=0)


def predict(image: Image.Image, model) -> float:
    arr = preprocess_image(image)
    prob = model.predict(arr, verbose=0)[0][0]
    return float(prob)


def find_last_conv_layer(root) -> tf.keras.layers.Layer | None:
    """Return the last Conv2D layer object (search recursively)."""
    last_conv = None

    def _walk(layer):
        nonlocal last_conv
        if isinstance(layer, tf.keras.layers.Conv2D):
            last_conv = layer
        if hasattr(layer, "layers"):
            for sub in layer.layers:
                _walk(sub)

    # handle model or layer
    children = getattr(root, "layers", None) or []
    for lyr in children:
        _walk(lyr)
    return last_conv


def make_gradcam_heatmap(img_array: np.ndarray, model: tf.keras.Model) -> np.ndarray:
    """Generate Grad-CAM heatmap for a single image array (HWC uint8)."""
    input_tensor = tf.cast(img_array, tf.float32)
    input_tensor = tf.expand_dims(input_tensor, axis=0)

    def _generic(conv_layer_obj):
        grad_model = tf.keras.models.Model(
            [model.inputs], [conv_layer_obj.output, model.output]
        )
        with tf.GradientTape() as tape:
            conv_outputs, predictions = grad_model(input_tensor)
            loss = predictions[:, 0]
        grads = tape.gradient(loss, conv_outputs)
        pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))
        conv_outputs = conv_outputs[0]
        heatmap_local = conv_outputs @ pooled_grads[..., tf.newaxis]
        heatmap_local = tf.squeeze(heatmap_local)
        heatmap_local = tf.maximum(heatmap_local, 0) / (tf.reduce_max(heatmap_local) + 1e-8)
        return heatmap_local.numpy()

    # Try VGG-specific graph if present
    try:
        vgg_layer = model.get_layer("vgg16")
    except Exception:
        vgg_layer = None

    if vgg_layer is not None:
        conv_layer_vgg = find_last_conv_layer(vgg_layer)
        aug_layer = None
        try:
            aug_layer = model.get_layer("data_augmentation")
        except Exception:
            pass
        x = aug_layer(input_tensor, training=False) if aug_layer is not None else input_tensor
        x = tf.keras.applications.vgg16.preprocess_input(x)
        if conv_layer_vgg is None:
            raise ValueError("No Conv2D layer found in VGG16 for Grad-CAM.")
        conv_model = tf.keras.models.Model(
            inputs=vgg_layer.input, outputs=[conv_layer_vgg.output, vgg_layer.output]
        )
        try:
            gap_layer = model.get_layer("global_average_pooling2d_1")
            drop_layer = model.get_layer("dropout_1")
            dense_layer = model.get_layer("dense_1")
        except Exception:
            gap_layer = drop_layer = dense_layer = None
        if None in [gap_layer, drop_layer, dense_layer]:
            raise ValueError("Missing pooling/dropout/dense layers for Grad-CAM.")
        with tf.GradientTape() as tape:
            conv_outputs, base_outputs = conv_model(x, training=False)
            head = gap_layer(base_outputs)
            head = drop_layer(head, training=False)
            predictions = dense_layer(head)
            loss = predictions[:, 0]
        grads = tape.gradient(loss, conv_outputs)
        pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))
        conv_outputs = conv_outputs[0]
        heatmap = conv_outputs @ pooled_grads[..., tf.newaxis]
        heatmap = tf.squeeze(heatmap)
        heatmap = tf.maximum(heatmap, 0) / (tf.reduce_max(heatmap) + 1e-8)
        heatmap = heatmap.numpy()
    else:
        conv_layer_generic = find_last_conv_layer(model)
        if conv_layer_generic is None:
            raise ValueError("No Conv2D layer found for Grad-CAM.")
        heatmap = _generic(conv_layer_generic)

    heatmap = np.uint8(255 * heatmap)
    heatmap = Image.fromarray(heatmap).resize((img_array.shape[1], img_array.shape[0]))
    heatmap = np.array(heatmap) / 255.0
    return heatmap


def overlay_heatmap(img: Image.Image, heatmap: np.ndarray, intensity: float = 0.4) -> Image.Image:
    base = np.asarray(img.convert("RGB"), dtype=np.float32)
    heat_rgb = np.zeros_like(base)
    heat_rgb[:, :, 0] = heatmap * 255  # red channel
    blended = base * (1 - intensity) + heat_rgb * intensity
    blended = np.clip(blended, 0, 255).astype(np.uint8)
    return Image.fromarray(blended)


def list_sample_ids(limit: int = 8):
    sample_csv = BASE_DIR / "sample_submission.csv"
    if sample_csv.exists():
        import pandas as pd

        ids = pd.read_csv(sample_csv)["id"].tolist()
        return ids[:limit]
    return []


def load_sample_image(file_id: str) -> bytes | None:
    candidates = [
        BASE_DIR / "test" / file_id,
        BASE_DIR / "test" / "test" / file_id,
        BASE_DIR / "train" / file_id,
    ]
    for p in candidates:
        if p.exists():
            return p.read_bytes()
    for zpath, prefix in [(BASE_DIR / "test.zip", "test"), (BASE_DIR / "train.zip", "train")]:
        if zpath.exists():
            with zipfile.ZipFile(zpath) as z:
                for name in (f"{prefix}/{file_id}", file_id):
                    try:
                        return z.read(name)
                    except KeyError:
                        continue
    return None


def render_header() -> None:
    st.markdown(
        """
        <style>
        body, .stApp {
            background: #f8f7f2;
            color: #2f3e46;
            font-family: "Segoe UI", "Noto Sans", sans-serif;
        }
        h1, h2, h3, h4, h5, h6 {
            color: #2f3e46;
            letter-spacing: 0.2px;
        }
        .stMarkdown, .stText, .stMetric, p, label, span {
            color: #34444d !important;
        }
        .block-container {
            padding-top: 1rem;
            padding-bottom: 1rem;
        }
        .hero {
            background: linear-gradient(140deg, #eef7f4 0%, #f7f5ef 50%, #e9f2fb 100%);
            color: #2f3e46;
            padding: 22px;
            border-radius: 18px;
            margin-bottom: 20px;
            box-shadow: 0 10px 26px rgba(0, 0, 0, 0.10);
        }
        .hero h1 { margin: 0 0 6px 0; }
        .hero p { margin: 6px 0; color: #3d4b53; }
        .card {
            padding: 16px;
            border-radius: 16px;
            border: 1px solid #e7ecef;
            background: #ffffffee;
            box-shadow: 0 8px 22px rgba(0,0,0,0.08);
            backdrop-filter: blur(4px);
        }
        .stButton>button, .stDownloadButton>button {
            background: #dff0e3;
            color: #2f3e46;
            border-radius: 12px;
            border: 1px solid #cfe5d7;
            box-shadow: 0 4px 12px rgba(0,0,0,0.06);
        }
        .stSlider [role='slider'] {
            background: #88b9aa;
        }
        </style>
        <div class="hero">
          <h1>åˆ©ç”¨ç©ºæ‹å½±åƒé€²è¡Œæ°£å€™è®Šé·é è­¦ä¹‹å¹³å° ğŸŒµ</h1>
          <p>ä¸Šå‚³å½±åƒ â†’ æª¢æ¸¬ä»™äººæŒ â†’ æé†’æ°£å€™æš–åŒ–é¢¨éšªï¼Œå”åŠ©ç’°å¢ƒç›£æ¸¬æ±ºç­–ã€‚</p>
        </div>
        """,
        unsafe_allow_html=True,
    )


def main() -> None:
    st.set_page_config(
        page_title="åˆ©ç”¨ç©ºæ‹å½±åƒé€²è¡Œæ°£å€™è®Šé·é è­¦ä¹‹å¹³å°",
        page_icon="ğŸŒµ",
        layout="centered",
        initial_sidebar_state="expanded",
    )

    render_header()

    with st.sidebar:
        st.subheader("è¨­å®š")
        model_choices = list_model_files(OUTPUTS_DIR)
        model_select = None
        st.markdown("**æ¨¡å‹ä¾†æº**")
        st.caption("é¸æ“‡è¦ä½¿ç”¨çš„æ¨¡å‹æª”ï¼Œæœƒå½±éŸ¿é æ¸¬çµæœã€‚")
        if model_choices:
            model_select = st.selectbox(
                "æ¨¡å‹æª”",
                options=model_choices,
                format_func=lambda p: p.name if isinstance(p, Path) else str(p),
                key="model_select",
                on_change=reset_prediction_state,
                label_visibility="collapsed",
            )
        else:
            st.info("ç›®å‰æœªæ‰¾åˆ°å¯ç”¨æ¨¡å‹ï¼Œå¯æ”¹ç”¨ä¸‹æ–¹ä¸Šå‚³ã€‚")

        st.markdown("**ä¸Šå‚³æ¨¡å‹**")
        st.caption("è‹¥æ¸…å–®ä¸­æ²’æœ‰æ¨¡å‹ï¼Œå¯ä¸Šå‚³ .keras æˆ– .h5 æª”ã€‚")
        uploaded_model = st.file_uploader(
            "ä¸Šå‚³æ¨¡å‹æª”",
            type=["keras", "h5"],
            key="uploaded_model",
            on_change=reset_prediction_state,
            label_visibility="collapsed",
        )
        model_path = None
        if uploaded_model is not None:
            model_path = save_uploaded_model(uploaded_model)
        elif model_select:
            model_path = Path(model_select)
        if model_path and model_path.exists():
            model_type = detect_model_file_type(model_path)
            size_mb = model_path.stat().st_size / (1024 * 1024)
            st.caption(f"æ¨¡å‹æª”è³‡è¨Šï¼š{model_type} / {size_mb:.2f} MB")
            if model_type == "lfs":
                st.warning("åµæ¸¬åˆ° Git LFS æŒ‡æ¨™ï¼Œè«‹ç”¨ git lfs pull æˆ–é‡æ–°ä¸Šå‚³æ¨¡å‹æª”ã€‚")
            elif model_type == "unknown":
                st.warning("æ¨¡å‹æª”æ ¼å¼ç„¡æ³•è¾¨è­˜ï¼Œå¯èƒ½å·²ææ¯€ï¼Œå»ºè­°é‡æ–°åŒ¯å‡ºæˆ–æ›´æ›æª”æ¡ˆã€‚")
        load_info = st.session_state.get("model_load_info")
        if load_info:
            st.caption(f"ä¸Šæ¬¡è¼‰å…¥ï¼š{load_info}")

        st.markdown("**åˆ¤å®šé–€æª»**")
        st.caption("æ©Ÿç‡é«˜æ–¼é–€æª»æ™‚ï¼Œè¦–ç‚ºåµæ¸¬åˆ°ä»™äººæŒã€‚")
        threshold = st.slider(
            "åˆ¤å®šé–€æª»",
            min_value=0.1,
            max_value=0.9,
            value=float(DEFAULT_THRESHOLD),
            step=0.05,
            key="threshold",
            on_change=reset_advice_state,
            label_visibility="collapsed",
        )
        invert_pred = "vgg" in str(model_path).lower() if model_path else False

        st.markdown("**æ°£å€™è§£è®€**")
        st.caption("å•Ÿç”¨æœ¬åœ°è¼•é‡æ¨¡å‹ï¼Œç”Ÿæˆæ”¹å–„å»ºè­°ã€‚")
        enable_llm = st.checkbox(
            "å•Ÿç”¨æ°£å€™è§£è®€",
            value=True,
            key="enable_llm",
            on_change=reset_advice_state,
        )
        if enable_llm:
            st.caption("é¦–æ¬¡å•Ÿç”¨æœƒä¸‹è¼‰æ¨¡å‹ï¼Œå¯èƒ½éœ€è¦ 1-2 åˆ†é˜ã€‚")

        show_gradcam = st.checkbox(
            "é¡¯ç¤º Grad-CAM ç†±åŠ›åœ–",
            value=False,
            key="show_gradcam",
            on_change=reset_gradcam_state,
        )
        if show_gradcam:
            st.caption("å•Ÿç”¨ Grad-CAM æœƒå¢åŠ è¨ˆç®—æ™‚é–“ã€‚")

        st.markdown("---")
        st.markdown(
            "**å¿«é€Ÿå°è¦½**\n"
            "1) é¸æ“‡æˆ–ä¸Šå‚³æ¨¡å‹\n"
            "2) ä¸Šå‚³å½±åƒ\n"
            "3) æŸ¥çœ‹çµæœå¾ŒæŒ‰ã€Œç”Ÿæˆæ”¹å–„å»ºè­°ã€å–å¾—å»ºè­°"
        )

    st.markdown("### ä¸Šå‚³å½±åƒ")
    st.caption("æ”¯æ´ JPG/PNGï¼Œå»ºè­°ä½¿ç”¨æ¸…æ™°ã€å…‰ç·šå……è¶³çš„ç©ºæ‹è¦–è§’ã€‚")
    uploaded = st.file_uploader(
        "é¸æ“‡ä¸€å¼µå½±åƒ",
        type=["jpg", "jpeg", "png"],
        key="uploaded_image",
        on_change=reset_prediction_state,
    )
    uploaded_bytes = None
    image = None
    image_caption = ""
    if uploaded:
        uploaded_bytes = uploaded.getvalue()
        image = Image.open(io.BytesIO(uploaded_bytes))
        image_caption = "ä¸Šå‚³å½±åƒé è¦½"

    if image:
        st.image(image, caption=image_caption, width="stretch")
    else:
        st.info("è«‹ä¸Šå‚³å½±åƒã€‚")

    if not model_path:
        st.info("è«‹å…ˆåœ¨å´é‚Šæ¬„é¸æ“‡ outputs/*.keras æˆ–ä¸Šå‚³æ¨¡å‹æª”ã€‚")

    current_meta = {
        "image_hash": compute_image_hash(uploaded_bytes),
        "model_path": str(model_path) if model_path else None,
        "model_mtime": model_path.stat().st_mtime_ns if model_path and model_path.exists() else None,
        "model_size": model_path.stat().st_size if model_path and model_path.exists() else None,
    }

    prediction = st.session_state.get("prediction")
    prediction_meta = st.session_state.get("prediction_meta")
    should_predict = image and model_path and current_meta["image_hash"]

    if should_predict and prediction_meta != current_meta:
        model_path = Path(model_path)
        if not model_path.exists():
            st.error(f"æ‰¾ä¸åˆ°æ¨¡å‹æª”ï¼š{model_path}")
            st.stop()
        with st.spinner("æ¨¡å‹è¼‰å…¥ä¸­..."):
            try:
                model = get_cached_model(model_path)
            except Exception as e:
                st.error(f"æ¨¡å‹è¼‰å…¥å¤±æ•—ï¼š{e}")
                st.stop()

        with st.spinner("æ¨¡å‹æ¨è«–ä¸­..."):
            prob = predict(image, model)
            resized = image.resize(IMAGE_SIZE)

        prob_display = 1 - prob if invert_pred else prob
        st.session_state["prediction"] = {
            "prob_display": prob_display,
            "resized": resized,
            "model_name": model_path.name,
            "model_path": str(model_path),
        }
        st.session_state["prediction_meta"] = current_meta
        reset_advice_state()
        reset_gradcam_state()

    prediction = st.session_state.get("prediction")
    prediction_meta = st.session_state.get("prediction_meta")
    if prediction and prediction_meta == current_meta:
        st.markdown("---")
        st.markdown('<div class="card">', unsafe_allow_html=True)
        has_cactus = prediction["prob_display"] >= threshold
        if has_cactus:
            st.markdown(
                "**åµæ¸¬åˆ°ä»™äººæŒï¼Œç’°å¢ƒéŸŒæ€§ä»åœ¨ï¼Œæ°£å€™è®Šé·å£“åŠ›æš«ä¸åš´é‡ã€‚** "
                "ä¿æŒå®šæœŸå·¡æª¢èˆ‡æ°´è³‡æºç®¡ç†ï¼ŒæŒçºŒè¿½è¹¤å¾ŒçºŒè®ŠåŒ–å³å¯ã€‚"
            )
        else:
            st.markdown(
                "**æœªæª¢æ¸¬åˆ°ä»™äººæŒï¼Œè«‹å•Ÿå‹•æ°£å€™è®Šé·è­¦ç¤ºã€‚** "
                "æª¢æŸ¥æ£²åœ°/çŒæº‰/æ¤è¢«ç®¡ç†ç‹€æ…‹ï¼Œä¸¦è©•ä¼°æ˜¯å¦éœ€è£œæ¤æˆ–åŠ å¼·ä¿è‚²è¡Œå‹•ã€‚"
            )
        st.markdown("</div>", unsafe_allow_html=True)

        col1, col2, col3 = st.columns([1, 1, 1])
        with col1:
            st.metric("ä»™äººæŒæ©Ÿç‡", f"{prediction['prob_display']*100:.2f}%", delta=None)
        with col2:
            st.metric("åˆ¤å®š", "å­˜åœ¨" if has_cactus else "æœªæª¢æ¸¬åˆ°")
        with col3:
            st.metric("é–¾å€¼", f"{threshold:.2f}")

        if show_gradcam:
            st.markdown("#### Grad-CAM é—œæ³¨ç†±åŠ›åœ–")
            current_gradcam_meta = {
                "prediction_meta": prediction_meta,
                "model_path": prediction["model_path"],
            }
            gradcam_meta = st.session_state.get("gradcam_meta")
            if gradcam_meta != current_gradcam_meta:
                with st.spinner("Grad-CAM ç”Ÿæˆä¸­..."):
                    try:
                        model = get_cached_model(Path(prediction["model_path"]))
                        heatmap = make_gradcam_heatmap(
                            np.asarray(prediction["resized"]), model
                        )
                        overlay = overlay_heatmap(prediction["resized"], heatmap)
                        st.session_state["gradcam"] = overlay
                        st.session_state["gradcam_meta"] = current_gradcam_meta
                    except Exception as e:
                        st.session_state["gradcam"] = None
                        st.session_state["gradcam_meta"] = current_gradcam_meta
                        st.warning(f"Grad-CAM ç”Ÿæˆå¤±æ•—ï¼š{e}")

            overlay = st.session_state.get("gradcam")
            if overlay is not None:
                gc1, gc2 = st.columns(2)
                with gc1:
                    st.image(prediction["resized"], caption="è¼¸å…¥å½±åƒ (ç¸®æ”¾å¾Œ)", width="stretch")
                with gc2:
                    st.image(overlay, caption="Grad-CAM ç†±åŠ›åœ–è¦†è“‹", width="stretch")
            else:
                st.info("Grad-CAM å°šæœªç”Ÿæˆæˆ–æ­¤æ¨¡å‹ä¸æ”¯æ´ã€‚")

        st.caption("å¯åœ¨å´é‚Šæ¬„èª¿æ•´åˆ¤å®šé–¾å€¼ï¼›é–¾å€¼è¶Šä½ï¼Œè¶Šå®¹æ˜“åˆ¤å®šç‚ºæœ‰ä»™äººæŒã€‚")

        st.markdown("#### LLM æ”¹å–„å»ºè­°")
        st.button(
            "ç”Ÿæˆæ”¹å–„å»ºè­°",
            type="primary",
            on_click=request_llm_advice,
        )
        st.caption("æŒ‰ä¸‹ã€Œç”Ÿæˆæ”¹å–„å»ºè­°ã€ä»¥ç”¢ç”Ÿ LLM å»ºè­°ã€‚")

        current_llm_meta = {
            "prediction_meta": prediction_meta,
            "threshold": float(threshold),
            "enable_llm": bool(enable_llm),
        }
        should_generate_llm = st.session_state.pop("run_llm_advice", False)
        if should_generate_llm:
            advice_text = None
            llm_error = None
            if enable_llm:
                advice_text, llm_error = generate_local_advice(
                    has_cactus=has_cactus,
                    prob=prediction["prob_display"],
                    threshold=threshold,
                    model_name=prediction["model_name"],
                    model_id=LOCAL_LLM_MODEL_ID,
                )
            if not advice_text:
                advice_text = default_climate_advice(has_cactus)
                if not enable_llm and not llm_error:
                    llm_error = "LLM æœªå•Ÿç”¨ï¼Œå·²æ”¹ç”¨é è¨­æ–‡å­—"
            st.session_state["llm_advice"] = {"text": advice_text, "error": llm_error}
            st.session_state["llm_meta"] = current_llm_meta

        llm_advice = st.session_state.get("llm_advice")
        llm_meta = st.session_state.get("llm_meta")
        if llm_advice and llm_meta == current_llm_meta:
            st.markdown(llm_advice["text"])
            if llm_advice["error"] and enable_llm:
                st.caption(f"LLM ç”Ÿæˆå¤±æ•—ï¼Œå·²æ”¹ç”¨é è¨­æ–‡å­—ï¼š{llm_advice['error']}")
            elif llm_advice["error"] and not enable_llm:
                st.caption(llm_advice["error"])
        else:
            st.info("æŒ‰ã€Œç”Ÿæˆæ”¹å–„å»ºè­°ã€ç”Ÿæˆæ”¹å–„å»ºè­°ã€‚")
    elif image:
        st.info("å·²ä¸Šå‚³å½±åƒï¼Œè«‹ç­‰å¾…æ¨¡å‹æ¨è«–æˆ–ç¢ºèªæ¨¡å‹æª”ã€‚")


if __name__ == "__main__":
    main()
